# Document Digitization MVP - Phase 1 Implementation Specification

## Project Overview
Build a functional MVP that accepts bulk PDF uploads, uses OCR to extract text, employs AI to categorize documents, and delivers organized results back to the user.

**Timeline**: 2-4 weeks  
**Success Criteria**: 80%+ categorization accuracy, process 50 documents in <10 minutes

---

## üéØ MVP Strategy: Start Free, Scale Smart

**This MVP uses Tesseract OCR (free) instead of Google Vision API ($$$)**

**Why?**
- ‚úÖ **Zero cost** during validation phase
- ‚úÖ **85-90% accuracy** is good enough to prove the concept  
- ‚úÖ **No cloud account setup** - just install Tesseract locally
- ‚úÖ **5-minute upgrade path** to Google Vision once you have paying customers

**When to upgrade:** After 2-3 paying customers generating $5K+/month

This keeps your MVP costs at ~$0.40 per 1,000 documents instead of $2.00, letting you validate the business model without upfront investment.

---

## Technology Stack

### Core Technologies
- **Backend**: Python 3.11+ with FastAPI
- **Frontend**: Simple HTML/CSS/JavaScript (vanilla or React if preferred)
- **OCR**: Tesseract (open source, free) - upgradeable to Google Vision API once you have paying customers
- **AI**: OpenAI GPT-4o-mini (cost-effective) or Claude Haiku
- **Storage**: Local filesystem for MVP (cloud migration in Phase 2)
- **Processing**: Python async for concurrent document processing

**Note**: Starting with Tesseract keeps costs at $0 during validation. Once you have 2-3 paying customers, upgrading to Google Vision API takes 5 minutes and improves accuracy from 85-90% to 99%.

### OCR Strategy: Start Free, Upgrade When Profitable

**Why Tesseract First?**
1. **Zero cost** during MVP validation
2. **85-90% accuracy** is good enough to prove the concept
3. **No setup complexity** - no cloud accounts or API keys needed
4. **Perfect for testing** - validate business model before spending money
5. **Easy upgrade** - switch to Google Vision in 5 minutes when ready

**When to Upgrade to Google Vision API:**
- You have 2-3 paying customers
- You're generating $5K+ monthly revenue  
- Clients request higher accuracy
- Processing volumes justify the $1.50/1000 pages cost

**The upgrade is literally just:**
1. `pip install google-cloud-vision`
2. Set `USE_GOOGLE_VISION=true` in `.env`
3. Restart the app

That's it. No code changes required.

### Dependencies
```python
# requirements.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6
aiofiles==23.2.1
pytesseract==0.3.10
openai==1.3.0
anthropic==0.7.0
PyPDF2==3.0.1
pdf2image==1.16.3
Pillow==10.1.0
python-dotenv==1.0.0
pydantic==2.5.0
pytest==7.4.3
pytest-asyncio==0.21.1

# System dependencies (install via apt/brew):
# - tesseract-ocr
# - poppler-utils (for pdf2image)
```

**Installation Notes:**
```bash
# Ubuntu/Debian
sudo apt-get install tesseract-ocr poppler-utils

# macOS
brew install tesseract poppler

# Windows
# Download Tesseract installer from: https://github.com/UB-Mannheim/tesseract/wiki
# Download Poppler from: https://github.com/oschwartz10612/poppler-windows/releases
```

---

## Project Structure

```
document-digitization-mvp/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI application entry point
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuration and environment variables
‚îÇ   ‚îú‚îÄ‚îÄ models.py               # Pydantic models for data validation
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ upload.py           # File upload endpoints
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ocr_service.py      # OCR processing logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai_service.py       # AI categorization logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ file_service.py     # File organization and management
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_utils.py        # PDF manipulation utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validators.py       # Input validation helpers
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ test_ocr.py
‚îÇ       ‚îú‚îÄ‚îÄ test_ai.py
‚îÇ       ‚îî‚îÄ‚îÄ test_integration.py
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ index.html              # Main upload interface
‚îÇ   ‚îú‚îÄ‚îÄ styles.css              # Styling
‚îÇ   ‚îú‚îÄ‚îÄ app.js                  # Frontend logic
‚îÇ   ‚îî‚îÄ‚îÄ results.html            # Results display page
‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îú‚îÄ‚îÄ uploads/                # Temporary upload storage
‚îÇ   ‚îú‚îÄ‚îÄ processed/              # Organized documents by category
‚îÇ   ‚îî‚îÄ‚îÄ logs/                   # Processing logs
‚îú‚îÄ‚îÄ .env.example                # Environment variables template
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md                   # Setup and usage instructions
‚îî‚îÄ‚îÄ docker-compose.yml          # Optional: containerized deployment
```

---

## Implementation Steps

### Step 1: Project Setup and Configuration

#### 1.1 Environment Configuration
Create `.env` file with required API keys:
```bash
# API Keys
OPENAI_API_KEY=sk-...

# Alternative: Use Claude instead
ANTHROPIC_API_KEY=sk-ant-...

# OCR Settings
USE_GOOGLE_VISION=false  # Set to 'true' once you have paying customers and want to upgrade
GOOGLE_APPLICATION_CREDENTIALS=  # Leave empty for now, add path later when upgrading

# Application Settings
UPLOAD_DIR=./storage/uploads
PROCESSED_DIR=./storage/processed
LOG_DIR=./storage/logs
MAX_FILE_SIZE=50  # MB
ALLOWED_EXTENSIONS=pdf
MAX_CONCURRENT_PROCESSING=5

# Server Settings
HOST=0.0.0.0
PORT=8000
```

#### 1.2 Configuration Module (`backend/config.py`)
```python
from pydantic_settings import BaseSettings
from typing import List
import os

class Settings(BaseSettings):
    # API Keys
    openai_api_key: str
    anthropic_api_key: str | None = None
    
    # OCR Settings
    use_google_vision: bool = False
    google_application_credentials: str | None = None
    
    # Directories
    upload_dir: str = "./storage/uploads"
    processed_dir: str = "./storage/processed"
    log_dir: str = "./storage/logs"
    
    # File Settings
    max_file_size: int = 50  # MB
    allowed_extensions: List[str] = ["pdf"]
    max_concurrent_processing: int = 5
    
    # Server
    host: str = "0.0.0.0"
    port: int = 8000
    
    class Config:
        env_file = ".env"
        
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # Create directories if they don't exist
        os.makedirs(self.upload_dir, exist_ok=True)
        os.makedirs(self.processed_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)

settings = Settings()
```

---

### Step 2: Data Models

#### 2.1 Request/Response Models (`backend/models.py`)
```python
from pydantic import BaseModel, Field
from typing import List, Optional
from enum import Enum
from datetime import datetime

class DocumentCategory(str, Enum):
    INVOICE = "Invoice"
    CONTRACT = "Contract"
    RECEIPT = "Receipt"
    LEGAL = "Legal Document"
    HR = "HR Document"
    TAX = "Tax Document"
    FINANCIAL = "Financial Statement"
    CORRESPONDENCE = "Correspondence"
    OTHER = "Other"

class ProcessingStatus(str, Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class DocumentResult(BaseModel):
    filename: str
    original_path: str
    category: DocumentCategory
    confidence: float = Field(ge=0.0, le=1.0)
    processed_path: Optional[str] = None
    extracted_text_preview: str  # First 500 chars
    error: Optional[str] = None
    processing_time: float  # seconds

class BatchUploadResponse(BaseModel):
    batch_id: str
    total_files: int
    status: ProcessingStatus
    started_at: datetime
    
class BatchResultResponse(BaseModel):
    batch_id: str
    status: ProcessingStatus
    total_files: int
    processed_files: int
    successful: int
    failed: int
    results: List[DocumentResult]
    processing_summary: dict
    download_url: Optional[str] = None
```

---

### Step 3: OCR Service

#### 3.1 OCR Service Implementation (`backend/services/ocr_service.py`)
```python
import os
import pytesseract
from pdf2image import convert_from_path
from PIL import Image
import io
from typing import Optional
from backend.config import settings

class OCRService:
    def __init__(self):
        """
        Initialize OCR service.
        Uses Tesseract by default, with option to upgrade to Google Vision API later.
        """
        self.use_google = settings.use_google_vision
        
        if self.use_google:
            try:
                from google.cloud import vision
                self.vision_client = vision.ImageAnnotatorClient()
                print("‚úì Google Vision API initialized")
            except Exception as e:
                print(f"‚ö† Google Vision API not available, falling back to Tesseract: {e}")
                self.use_google = False
        else:
            print("‚úì Using Tesseract OCR (free)")
    
    async def extract_text_from_pdf(self, pdf_path: str) -> str:
        """
        Extract text from PDF using OCR.
        Uses Tesseract by default, Google Vision if enabled in settings.
        """
        try:
            # Convert PDF to images (limit to first 5 pages for MVP)
            images = convert_from_path(pdf_path, dpi=300, first_page=1, last_page=5)
            
            all_text = []
            
            for i, image in enumerate(images):
                if self.use_google:
                    text = await self._google_ocr(image)
                else:
                    text = await self._tesseract_ocr(image)
                    
                if text:
                    all_text.append(text)
            
            return "\n\n".join(all_text)
            
        except Exception as e:
            raise Exception(f"OCR processing failed: {str(e)}")
    
    async def _tesseract_ocr(self, image: Image.Image) -> str:
        """
        Use Tesseract for OCR (free, open source).
        85-90% accuracy - good enough for MVP.
        """
        try:
            # Optional: Enhance image for better OCR results
            # image = image.convert('L')  # Convert to grayscale
            # image = image.point(lambda x: 0 if x < 128 else 255, '1')  # Binarize
            
            text = pytesseract.image_to_string(image, lang='eng')
            return text
        except Exception as e:
            raise Exception(f"Tesseract OCR failed: {str(e)}")
    
    async def _google_ocr(self, image: Image.Image) -> str:
        """
        Use Google Vision API for OCR (paid, 99% accuracy).
        Enable this once you have paying customers.
        """
        try:
            from google.cloud import vision
            
            # Convert PIL Image to bytes
            img_byte_arr = io.BytesIO()
            image.save(img_byte_arr, format='PNG')
            img_byte_arr = img_byte_arr.getvalue()
            
            # Create Vision API image object
            vision_image = vision.Image(content=img_byte_arr)
            
            # Perform text detection
            response = self.vision_client.text_detection(image=vision_image)
            texts = response.text_annotations
            
            if texts:
                return texts[0].description
            return ""
            
        except Exception as e:
            print(f"Google OCR failed, falling back to Tesseract: {e}")
            return await self._tesseract_ocr(image)
    
    def validate_ocr_quality(self, text: str) -> bool:
        """
        Basic validation to ensure OCR produced meaningful text.
        Returns False if text is too short or mostly garbage.
        """
        if not text or len(text.strip()) < 50:
            return False
        
        # Check for reasonable word count
        words = text.split()
        if len(words) < 10:
            return False
            
        # Check that text isn't mostly special characters
        alphanumeric_ratio = sum(c.isalnum() or c.isspace() for c in text) / len(text)
        if alphanumeric_ratio < 0.6:
            return False
            
        return True
```

**Upgrade Path to Google Vision (When Ready):**

1. Install Google Cloud Vision:
   ```bash
   pip install google-cloud-vision==3.4.5
   ```

2. Set up Google Cloud:
   - Create project at console.cloud.google.com
   - Enable Vision API
   - Create service account and download JSON key

3. Update `.env`:
   ```bash
   USE_GOOGLE_VISION=true
   GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
   ```

4. Restart the application - that's it! üéâ

---

### Step 4: AI Categorization Service

#### 4.1 AI Service Implementation (`backend/services/ai_service.py`)
```python
import openai
from anthropic import Anthropic
from typing import Tuple
import json
from backend.models import DocumentCategory
from backend.config import settings

class AIService:
    def __init__(self, provider: str = "openai"):
        self.provider = provider
        
        if provider == "openai":
            openai.api_key = settings.openai_api_key
            self.model = "gpt-4o-mini"  # Cost-effective choice
        elif provider == "anthropic":
            self.client = Anthropic(api_key=settings.anthropic_api_key)
            self.model = "claude-3-haiku-20240307"  # Cost-effective choice
    
    async def categorize_document(self, text: str, filename: str) -> Tuple[DocumentCategory, float]:
        """
        Categorize document using AI and return category with confidence score.
        
        Returns:
            Tuple of (DocumentCategory, confidence_score)
        """
        prompt = self._build_categorization_prompt(text, filename)
        
        try:
            if self.provider == "openai":
                response = await self._categorize_openai(prompt)
            else:
                response = await self._categorize_anthropic(prompt)
            
            return self._parse_categorization_response(response)
            
        except Exception as e:
            print(f"AI categorization failed: {e}")
            return DocumentCategory.OTHER, 0.3
    
    def _build_categorization_prompt(self, text: str, filename: str) -> str:
        """Build the categorization prompt"""
        # Truncate text if too long (to save on API costs)
        max_chars = 4000
        if len(text) > max_chars:
            text = text[:max_chars] + "...[truncated]"
        
        categories_list = ", ".join([cat.value for cat in DocumentCategory])
        
        return f"""You are a document classification expert. Analyze the following document and categorize it.

FILENAME: {filename}

DOCUMENT TEXT:
{text}

INSTRUCTIONS:
1. Categorize this document into ONE of the following categories:
   {categories_list}

2. Provide a confidence score between 0.0 and 1.0

3. Use these guidelines:
   - Invoice: Bills, payment requests, vendor invoices
   - Contract: Legal agreements, service contracts, NDAs
   - Receipt: Payment receipts, purchase confirmations
   - Legal Document: Court documents, legal notices, regulations
   - HR Document: Employee records, performance reviews, job offers
   - Tax Document: Tax returns, W2s, 1099s, tax assessments
   - Financial Statement: Balance sheets, P&L statements, financial reports
   - Correspondence: Letters, emails, memos, general communication
   - Other: Anything that doesn't fit the above categories

4. Respond ONLY with valid JSON in this exact format (no other text):
{{
    "category": "Category Name",
    "confidence": 0.95,
    "reasoning": "Brief explanation"
}}

DO NOT include markdown code blocks or any other formatting. Output only the JSON object."""

    async def _categorize_openai(self, prompt: str) -> str:
        """Get categorization from OpenAI"""
        response = await openai.ChatCompletion.acreate(
            model=self.model,
            messages=[
                {"role": "system", "content": "You are a document classification expert. Always respond with valid JSON only."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,  # Low temperature for consistent results
            max_tokens=200
        )
        
        return response.choices[0].message.content
    
    async def _categorize_anthropic(self, prompt: str) -> str:
        """Get categorization from Anthropic Claude"""
        message = await self.client.messages.create(
            model=self.model,
            max_tokens=200,
            temperature=0.1,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        return message.content[0].text
    
    def _parse_categorization_response(self, response: str) -> Tuple[DocumentCategory, float]:
        """Parse AI response and extract category and confidence"""
        try:
            # Clean up response (remove markdown code blocks if present)
            response = response.strip()
            if response.startswith("```"):
                response = response.split("```")[1]
                if response.startswith("json"):
                    response = response[4:]
            response = response.strip()
            
            # Parse JSON
            data = json.loads(response)
            
            # Extract category
            category_str = data.get("category", "Other")
            confidence = float(data.get("confidence", 0.5))
            
            # Match to enum
            category = self._match_category(category_str)
            
            # Clamp confidence between 0 and 1
            confidence = max(0.0, min(1.0, confidence))
            
            return category, confidence
            
        except Exception as e:
            print(f"Failed to parse AI response: {e}")
            print(f"Response was: {response}")
            return DocumentCategory.OTHER, 0.3
    
    def _match_category(self, category_str: str) -> DocumentCategory:
        """Match string to DocumentCategory enum"""
        category_str = category_str.strip().lower()
        
        for category in DocumentCategory:
            if category.value.lower() == category_str:
                return category
        
        # Partial matching as fallback
        if "invoice" in category_str:
            return DocumentCategory.INVOICE
        elif "contract" in category_str:
            return DocumentCategory.CONTRACT
        elif "receipt" in category_str:
            return DocumentCategory.RECEIPT
        elif "legal" in category_str:
            return DocumentCategory.LEGAL
        elif "hr" in category_str or "human resource" in category_str:
            return DocumentCategory.HR
        elif "tax" in category_str:
            return DocumentCategory.TAX
        elif "financial" in category_str:
            return DocumentCategory.FINANCIAL
        elif "correspondence" in category_str or "letter" in category_str:
            return DocumentCategory.CORRESPONDENCE
        
        return DocumentCategory.OTHER
```

---

### Step 5: File Management Service

#### 5.1 File Service Implementation (`backend/services/file_service.py`)
```python
import os
import shutil
import zipfile
from pathlib import Path
from typing import List
import aiofiles
from datetime import datetime
from backend.models import DocumentResult, DocumentCategory
from backend.config import settings

class FileService:
    def __init__(self):
        self.processed_dir = settings.processed_dir
    
    async def organize_documents(self, results: List[DocumentResult]) -> str:
        """
        Organize processed documents into category folders and create a ZIP file.
        
        Returns:
            Path to the ZIP file
        """
        # Create timestamp for this batch
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        batch_folder = os.path.join(self.processed_dir, f"batch_{timestamp}")
        
        # Create category folders
        for category in DocumentCategory:
            category_path = os.path.join(batch_folder, category.value)
            os.makedirs(category_path, exist_ok=True)
        
        # Move files to appropriate categories
        for result in results:
            if result.processed_path is None and result.error is None:
                # Copy file to category folder
                dest_folder = os.path.join(batch_folder, result.category.value)
                dest_path = os.path.join(dest_folder, result.filename)
                
                # Add category prefix to filename
                name, ext = os.path.splitext(result.filename)
                new_filename = f"[{result.category.value}] {name}{ext}"
                dest_path = os.path.join(dest_folder, new_filename)
                
                shutil.copy2(result.original_path, dest_path)
                result.processed_path = dest_path
        
        # Create processing log
        await self._create_processing_log(batch_folder, results)
        
        # Create ZIP file
        zip_path = f"{batch_folder}.zip"
        await self._create_zip(batch_folder, zip_path)
        
        return zip_path
    
    async def _create_processing_log(self, batch_folder: str, results: List[DocumentResult]):
        """Create a detailed processing log file"""
        log_path = os.path.join(batch_folder, "PROCESSING_LOG.txt")
        
        async with aiofiles.open(log_path, 'w') as f:
            await f.write("=" * 80 + "\n")
            await f.write("DOCUMENT PROCESSING LOG\n")
            await f.write("=" * 80 + "\n\n")
            await f.write(f"Processing Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            await f.write(f"Total Documents: {len(results)}\n")
            
            # Summary by category
            await f.write("\n" + "-" * 80 + "\n")
            await f.write("SUMMARY BY CATEGORY\n")
            await f.write("-" * 80 + "\n")
            
            category_counts = {}
            for result in results:
                category_counts[result.category] = category_counts.get(result.category, 0) + 1
            
            for category in sorted(category_counts.keys(), key=lambda x: x.value):
                await f.write(f"{category.value}: {category_counts[category]} documents\n")
            
            # Detailed results
            await f.write("\n" + "-" * 80 + "\n")
            await f.write("DETAILED RESULTS\n")
            await f.write("-" * 80 + "\n\n")
            
            for i, result in enumerate(results, 1):
                await f.write(f"{i}. {result.filename}\n")
                await f.write(f"   Category: {result.category.value}\n")
                await f.write(f"   Confidence: {result.confidence:.2%}\n")
                await f.write(f"   Processing Time: {result.processing_time:.2f}s\n")
                if result.error:
                    await f.write(f"   ERROR: {result.error}\n")
                await f.write("\n")
    
    async def _create_zip(self, source_folder: str, zip_path: str):
        """Create ZIP file of the processed documents"""
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(source_folder):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, source_folder)
                    zipf.write(file_path, arcname)
    
    async def cleanup_old_files(self, days: int = 7):
        """Clean up files older than specified days"""
        cutoff_time = datetime.now().timestamp() - (days * 86400)
        
        for folder in [settings.upload_dir, settings.processed_dir]:
            for item in os.listdir(folder):
                item_path = os.path.join(folder, item)
                if os.path.getmtime(item_path) < cutoff_time:
                    if os.path.isfile(item_path):
                        os.remove(item_path)
                    elif os.path.isdir(item_path):
                        shutil.rmtree(item_path)
```

---

### Step 6: Main Processing Pipeline

#### 6.1 Upload Route (`backend/routes/upload.py`)
```python
from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks
from typing import List
import os
import uuid
from datetime import datetime
import asyncio
import time

from backend.models import (
    BatchUploadResponse,
    BatchResultResponse,
    DocumentResult,
    ProcessingStatus,
    DocumentCategory
)
from backend.services.ocr_service import OCRService
from backend.services.ai_service import AIService
from backend.services.file_service import FileService
from backend.config import settings

router = APIRouter()

# In-memory storage for batch results (use Redis/DB in production)
batch_results = {}

# Initialize services
ocr_service = OCRService()
ai_service = AIService(provider="openai")  # or "anthropic"
file_service = FileService()


@router.post("/upload", response_model=BatchUploadResponse)
async def upload_documents(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...)
):
    """
    Upload multiple PDF documents for processing.
    Processing happens in background.
    """
    # Validate files
    if len(files) == 0:
        raise HTTPException(status_code=400, detail="No files provided")
    
    if len(files) > 100:
        raise HTTPException(status_code=400, detail="Maximum 100 files per batch")
    
    # Create batch ID
    batch_id = str(uuid.uuid4())
    upload_folder = os.path.join(settings.upload_dir, batch_id)
    os.makedirs(upload_folder, exist_ok=True)
    
    # Save uploaded files
    file_paths = []
    for file in files:
        # Validate file extension
        if not file.filename.endswith('.pdf'):
            raise HTTPException(
                status_code=400,
                detail=f"Invalid file type: {file.filename}. Only PDF files allowed."
            )
        
        # Check file size (in production, do this during upload)
        file_path = os.path.join(upload_folder, file.filename)
        
        # Save file
        with open(file_path, 'wb') as f:
            content = await file.read()
            if len(content) > settings.max_file_size * 1024 * 1024:
                raise HTTPException(
                    status_code=400,
                    detail=f"File {file.filename} exceeds maximum size of {settings.max_file_size}MB"
                )
            f.write(content)
        
        file_paths.append(file_path)
    
    # Initialize batch result
    batch_results[batch_id] = {
        "status": ProcessingStatus.PENDING,
        "total_files": len(file_paths),
        "processed_files": 0,
        "results": [],
        "started_at": datetime.now()
    }
    
    # Start background processing
    background_tasks.add_task(process_batch, batch_id, file_paths)
    
    return BatchUploadResponse(
        batch_id=batch_id,
        total_files=len(file_paths),
        status=ProcessingStatus.PENDING,
        started_at=datetime.now()
    )


async def process_batch(batch_id: str, file_paths: List[str]):
    """Process all documents in the batch"""
    batch_results[batch_id]["status"] = ProcessingStatus.PROCESSING
    
    results = []
    
    # Process files with concurrency limit
    semaphore = asyncio.Semaphore(settings.max_concurrent_processing)
    
    async def process_with_semaphore(file_path):
        async with semaphore:
            return await process_single_document(file_path)
    
    # Process all files
    tasks = [process_with_semaphore(fp) for fp in file_paths]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Handle exceptions
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            processed_results.append(DocumentResult(
                filename=os.path.basename(file_paths[i]),
                original_path=file_paths[i],
                category=DocumentCategory.OTHER,
                confidence=0.0,
                extracted_text_preview="",
                error=str(result),
                processing_time=0.0
            ))
        else:
            processed_results.append(result)
    
    # Organize files and create ZIP
    try:
        zip_path = await file_service.organize_documents(processed_results)
        download_url = f"/download/{batch_id}"
    except Exception as e:
        print(f"Failed to organize documents: {e}")
        download_url = None
    
    # Update batch results
    successful = sum(1 for r in processed_results if r.error is None)
    failed = len(processed_results) - successful
    
    # Calculate summary by category
    category_summary = {}
    for result in processed_results:
        if result.error is None:
            category_summary[result.category.value] = category_summary.get(result.category.value, 0) + 1
    
    batch_results[batch_id].update({
        "status": ProcessingStatus.COMPLETED,
        "processed_files": len(processed_results),
        "results": processed_results,
        "successful": successful,
        "failed": failed,
        "processing_summary": category_summary,
        "download_url": download_url
    })


async def process_single_document(file_path: str) -> DocumentResult:
    """Process a single document through the pipeline"""
    start_time = time.time()
    filename = os.path.basename(file_path)
    
    try:
        # Step 1: OCR
        extracted_text = await ocr_service.extract_text_from_pdf(file_path)
        
        # Validate OCR quality
        if not ocr_service.validate_ocr_quality(extracted_text):
            raise Exception("OCR quality check failed - insufficient text extracted")
        
        # Step 2: AI Categorization
        category, confidence = await ai_service.categorize_document(extracted_text, filename)
        
        # Create preview (first 500 chars)
        text_preview = extracted_text[:500] if extracted_text else ""
        
        processing_time = time.time() - start_time
        
        return DocumentResult(
            filename=filename,
            original_path=file_path,
            category=category,
            confidence=confidence,
            extracted_text_preview=text_preview,
            error=None,
            processing_time=processing_time
        )
        
    except Exception as e:
        processing_time = time.time() - start_time
        return DocumentResult(
            filename=filename,
            original_path=file_path,
            category=DocumentCategory.OTHER,
            confidence=0.0,
            extracted_text_preview="",
            error=str(e),
            processing_time=processing_time
        )


@router.get("/status/{batch_id}", response_model=BatchResultResponse)
async def get_batch_status(batch_id: str):
    """Get the processing status of a batch"""
    if batch_id not in batch_results:
        raise HTTPException(status_code=404, detail="Batch not found")
    
    batch = batch_results[batch_id]
    
    return BatchResultResponse(
        batch_id=batch_id,
        status=batch["status"],
        total_files=batch["total_files"],
        processed_files=batch["processed_files"],
        successful=batch.get("successful", 0),
        failed=batch.get("failed", 0),
        results=batch.get("results", []),
        processing_summary=batch.get("processing_summary", {}),
        download_url=batch.get("download_url")
    )


@router.get("/download/{batch_id}")
async def download_results(batch_id: str):
    """Download the organized documents as a ZIP file"""
    from fastapi.responses import FileResponse
    
    if batch_id not in batch_results:
        raise HTTPException(status_code=404, detail="Batch not found")
    
    batch = batch_results[batch_id]
    if batch["status"] != ProcessingStatus.COMPLETED:
        raise HTTPException(status_code=400, detail="Batch processing not complete")
    
    # Find the ZIP file
    timestamp_pattern = "*"  # In production, store this in batch_results
    zip_files = list(Path(settings.processed_dir).glob(f"batch_*.zip"))
    
    if not zip_files:
        raise HTTPException(status_code=404, detail="Results file not found")
    
    # Return the most recent ZIP (in production, track this properly)
    latest_zip = max(zip_files, key=lambda p: p.stat().st_mtime)
    
    return FileResponse(
        path=str(latest_zip),
        filename=f"processed_documents_{batch_id}.zip",
        media_type="application/zip"
    )
```

---

### Step 7: FastAPI Main Application

#### 7.1 Main Application (`backend/main.py`)
```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from backend.routes import upload
from backend.config import settings

app = FastAPI(
    title="Document Digitization MVP",
    description="AI-powered document categorization service",
    version="1.0.0"
)

# CORS middleware for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify exact origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(upload.router, prefix="/api", tags=["documents"])

# Serve frontend static files
app.mount("/", StaticFiles(directory="frontend", html=True), name="frontend")


@app.get("/api/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "version": "1.0.0"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "backend.main:app",
        host=settings.host,
        port=settings.port,
        reload=True
    )
```

---

### Step 8: Frontend Interface

#### 8.1 HTML Upload Interface (`frontend/index.html`)
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document Digitization - Upload</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>üìÑ Document Digitization Service</h1>
            <p>Upload your documents and let AI organize them automatically</p>
        </header>

        <main>
            <div class="upload-section">
                <div class="upload-box" id="uploadBox">
                    <div class="upload-icon">üìÅ</div>
                    <h2>Drop PDF files here or click to browse</h2>
                    <p>Maximum 100 files, 50MB each</p>
                    <input type="file" id="fileInput" multiple accept=".pdf" hidden>
                    <button id="browseBtn" class="btn-primary">Browse Files</button>
                </div>

                <div id="fileList" class="file-list hidden"></div>

                <div id="uploadControls" class="upload-controls hidden">
                    <button id="uploadBtn" class="btn-success">Process Documents</button>
                    <button id="clearBtn" class="btn-secondary">Clear All</button>
                </div>
            </div>

            <div id="processingSection" class="processing-section hidden">
                <h2>Processing Documents...</h2>
                <div class="progress-bar">
                    <div id="progressFill" class="progress-fill"></div>
                </div>
                <p id="processingStatus">Initializing...</p>
            </div>

            <div id="resultsSection" class="results-section hidden">
                <h2>‚úÖ Processing Complete!</h2>
                <div id="resultsSummary" class="results-summary"></div>
                <div id="resultsDetails" class="results-details"></div>
                <button id="downloadBtn" class="btn-success">Download Organized Documents</button>
                <button id="newBatchBtn" class="btn-secondary">Process New Batch</button>
            </div>
        </main>

        <footer>
            <p>Powered by AI ‚Ä¢ Secure ‚Ä¢ Fast</p>
        </footer>
    </div>

    <script src="app.js"></script>
</body>
</html>
```

#### 8.2 Styling (`frontend/styles.css`)
```css
* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    min-height: 100vh;
    padding: 20px;
}

.container {
    max-width: 1000px;
    margin: 0 auto;
    background: white;
    border-radius: 16px;
    box-shadow: 0 20px 60px rgba(0,0,0,0.3);
    overflow: hidden;
}

header {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 40px;
    text-align: center;
}

header h1 {
    font-size: 2.5em;
    margin-bottom: 10px;
}

header p {
    font-size: 1.2em;
    opacity: 0.9;
}

main {
    padding: 40px;
}

.upload-box {
    border: 3px dashed #667eea;
    border-radius: 12px;
    padding: 60px 40px;
    text-align: center;
    transition: all 0.3s ease;
    cursor: pointer;
}

.upload-box:hover {
    border-color: #764ba2;
    background: #f8f9ff;
}

.upload-box.drag-over {
    border-color: #764ba2;
    background: #f0f0ff;
    transform: scale(1.02);
}

.upload-icon {
    font-size: 4em;
    margin-bottom: 20px;
}

.upload-box h2 {
    color: #333;
    margin-bottom: 10px;
}

.upload-box p {
    color: #666;
    margin-bottom: 20px;
}

.btn-primary, .btn-success, .btn-secondary {
    padding: 12px 30px;
    border: none;
    border-radius: 8px;
    font-size: 1em;
    font-weight: 600;
    cursor: pointer;
    transition: all 0.3s ease;
    margin: 5px;
}

.btn-primary {
    background: #667eea;
    color: white;
}

.btn-primary:hover {
    background: #5568d3;
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
}

.btn-success {
    background: #48bb78;
    color: white;
}

.btn-success:hover {
    background: #38a169;
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(72, 187, 120, 0.4);
}

.btn-secondary {
    background: #e2e8f0;
    color: #333;
}

.btn-secondary:hover {
    background: #cbd5e0;
}

.file-list {
    margin-top: 30px;
    max-height: 300px;
    overflow-y: auto;
    border: 1px solid #e2e8f0;
    border-radius: 8px;
    padding: 20px;
}

.file-item {
    display: flex;
    align-items: center;
    padding: 10px;
    background: #f7fafc;
    border-radius: 6px;
    margin-bottom: 8px;
}

.file-item-icon {
    font-size: 1.5em;
    margin-right: 10px;
}

.file-item-name {
    flex: 1;
    font-weight: 500;
    color: #333;
}

.file-item-size {
    color: #666;
    font-size: 0.9em;
}

.upload-controls {
    margin-top: 30px;
    text-align: center;
}

.processing-section {
    text-align: center;
    padding: 40px;
}

.progress-bar {
    width: 100%;
    height: 30px;
    background: #e2e8f0;
    border-radius: 15px;
    overflow: hidden;
    margin: 30px 0;
}

.progress-fill {
    height: 100%;
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    transition: width 0.3s ease;
    width: 0%;
}

.results-section {
    padding: 40px;
    text-align: center;
}

.results-summary {
    background: #f0fff4;
    border: 2px solid #48bb78;
    border-radius: 12px;
    padding: 30px;
    margin: 30px 0;
}

.summary-stat {
    display: inline-block;
    margin: 0 20px;
    text-align: center;
}

.summary-stat-number {
    font-size: 3em;
    font-weight: bold;
    color: #667eea;
}

.summary-stat-label {
    font-size: 1.1em;
    color: #666;
    margin-top: 5px;
}

.results-details {
    margin: 30px 0;
    text-align: left;
}

.category-group {
    background: #f7fafc;
    border-radius: 8px;
    padding: 20px;
    margin-bottom: 15px;
}

.category-header {
    font-size: 1.3em;
    font-weight: 600;
    color: #333;
    margin-bottom: 10px;
}

.category-count {
    color: #667eea;
    font-weight: bold;
}

.document-item {
    padding: 8px 0;
    border-bottom: 1px solid #e2e8f0;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

.document-item:last-child {
    border-bottom: none;
}

.document-name {
    font-weight: 500;
    color: #333;
}

.confidence-badge {
    padding: 4px 12px;
    border-radius: 12px;
    font-size: 0.85em;
    font-weight: 600;
}

.confidence-high {
    background: #c6f6d5;
    color: #22543d;
}

.confidence-medium {
    background: #feebc8;
    color: #7c2d12;
}

.confidence-low {
    background: #fed7d7;
    color: #742a2a;
}

.hidden {
    display: none !important;
}

footer {
    background: #f7fafc;
    padding: 20px;
    text-align: center;
    color: #666;
}
```

#### 8.3 Frontend Logic (`frontend/app.js`)
```javascript
const API_BASE = '/api';

let selectedFiles = [];
let currentBatchId = null;

// DOM Elements
const uploadBox = document.getElementById('uploadBox');
const fileInput = document.getElementById('fileInput');
const browseBtn = document.getElementById('browseBtn');
const fileList = document.getElementById('fileList');
const uploadControls = document.getElementById('uploadControls');
const uploadBtn = document.getElementById('uploadBtn');
const clearBtn = document.getElementById('clearBtn');
const processingSection = document.getElementById('processingSection');
const progressFill = document.getElementById('progressFill');
const processingStatus = document.getElementById('processingStatus');
const resultsSection = document.getElementById('resultsSection');
const resultsSummary = document.getElementById('resultsSummary');
const resultsDetails = document.getElementById('resultsDetails');
const downloadBtn = document.getElementById('downloadBtn');
const newBatchBtn = document.getElementById('newBatchBtn');

// Event Listeners
browseBtn.addEventListener('click', () => fileInput.click());
fileInput.addEventListener('change', handleFileSelect);
uploadBox.addEventListener('click', () => fileInput.click());
uploadBox.addEventListener('dragover', handleDragOver);
uploadBox.addEventListener('dragleave', handleDragLeave);
uploadBox.addEventListener('drop', handleDrop);
uploadBtn.addEventListener('click', uploadDocuments);
clearBtn.addEventListener('click', clearFiles);
downloadBtn.addEventListener('click', downloadResults);
newBatchBtn.addEventListener('click', resetApp);

// File Selection
function handleFileSelect(e) {
    const files = Array.from(e.target.files);
    addFiles(files);
}

function handleDragOver(e) {
    e.preventDefault();
    uploadBox.classList.add('drag-over');
}

function handleDragLeave(e) {
    e.preventDefault();
    uploadBox.classList.remove('drag-over');
}

function handleDrop(e) {
    e.preventDefault();
    uploadBox.classList.remove('drag-over');
    const files = Array.from(e.dataTransfer.files).filter(f => f.name.endsWith('.pdf'));
    addFiles(files);
}

function addFiles(files) {
    // Validate file count
    if (selectedFiles.length + files.length > 100) {
        alert('Maximum 100 files allowed per batch');
        return;
    }
    
    // Validate file types and sizes
    const validFiles = files.filter(file => {
        if (!file.name.endsWith('.pdf')) {
            alert(`${file.name} is not a PDF file`);
            return false;
        }
        if (file.size > 50 * 1024 * 1024) {
            alert(`${file.name} exceeds 50MB limit`);
            return false;
        }
        return true;
    });
    
    selectedFiles.push(...validFiles);
    renderFileList();
}

function renderFileList() {
    if (selectedFiles.length === 0) {
        fileList.classList.add('hidden');
        uploadControls.classList.add('hidden');
        return;
    }
    
    fileList.classList.remove('hidden');
    uploadControls.classList.remove('hidden');
    
    fileList.innerHTML = `
        <h3>Selected Files (${selectedFiles.length})</h3>
        ${selectedFiles.map((file, index) => `
            <div class="file-item">
                <span class="file-item-icon">üìÑ</span>
                <span class="file-item-name">${file.name}</span>
                <span class="file-item-size">${formatFileSize(file.size)}</span>
            </div>
        `).join('')}
    `;
}

function clearFiles() {
    selectedFiles = [];
    fileInput.value = '';
    renderFileList();
}

function formatFileSize(bytes) {
    if (bytes < 1024) return bytes + ' B';
    if (bytes < 1024 * 1024) return (bytes / 1024).toFixed(1) + ' KB';
    return (bytes / (1024 * 1024)).toFixed(1) + ' MB';
}

// Upload and Processing
async function uploadDocuments() {
    if (selectedFiles.length === 0) return;
    
    // Show processing section
    document.querySelector('.upload-section').classList.add('hidden');
    processingSection.classList.remove('hidden');
    
    try {
        // Create FormData
        const formData = new FormData();
        selectedFiles.forEach(file => {
            formData.append('files', file);
        });
        
        // Upload files
        processingStatus.textContent = 'Uploading files...';
        const response = await fetch(`${API_BASE}/upload`, {
            method: 'POST',
            body: formData
        });
        
        if (!response.ok) {
            throw new Error('Upload failed');
        }
        
        const data = await response.json();
        currentBatchId = data.batch_id;
        
        // Poll for results
        await pollBatchStatus();
        
    } catch (error) {
        alert('Upload failed: ' + error.message);
        resetApp();
    }
}

async function pollBatchStatus() {
    const pollInterval = 2000; // 2 seconds
    
    while (true) {
        try {
            const response = await fetch(`${API_BASE}/status/${currentBatchId}`);
            const data = await response.json();
            
            // Update progress
            const progress = (data.processed_files / data.total_files) * 100;
            progressFill.style.width = progress + '%';
            processingStatus.textContent = `Processing: ${data.processed_files} of ${data.total_files} documents...`;
            
            if (data.status === 'completed') {
                showResults(data);
                break;
            }
            
            await new Promise(resolve => setTimeout(resolve, pollInterval));
            
        } catch (error) {
            console.error('Polling error:', error);
            await new Promise(resolve => setTimeout(resolve, pollInterval));
        }
    }
}

function showResults(data) {
    processingSection.classList.add('hidden');
    resultsSection.classList.remove('hidden');
    
    // Summary
    resultsSummary.innerHTML = `
        <div class="summary-stat">
            <div class="summary-stat-number">${data.successful}</div>
            <div class="summary-stat-label">Successfully Processed</div>
        </div>
        <div class="summary-stat">
            <div class="summary-stat-number">${data.failed}</div>
            <div class="summary-stat-label">Failed</div>
        </div>
        <div class="summary-stat">
            <div class="summary-stat-number">${Object.keys(data.processing_summary).length}</div>
            <div class="summary-stat-label">Categories</div>
        </div>
    `;
    
    // Group results by category
    const byCategory = {};
    data.results.forEach(result => {
        if (!result.error) {
            if (!byCategory[result.category]) {
                byCategory[result.category] = [];
            }
            byCategory[result.category].push(result);
        }
    });
    
    // Render categories
    resultsDetails.innerHTML = Object.keys(byCategory).map(category => `
        <div class="category-group">
            <div class="category-header">
                ${category} <span class="category-count">(${byCategory[category].length})</span>
            </div>
            ${byCategory[category].map(doc => `
                <div class="document-item">
                    <span class="document-name">${doc.filename}</span>
                    <span class="confidence-badge ${getConfidenceClass(doc.confidence)}">
                        ${(doc.confidence * 100).toFixed(0)}% confident
                    </span>
                </div>
            `).join('')}
        </div>
    `).join('');
}

function getConfidenceClass(confidence) {
    if (confidence >= 0.8) return 'confidence-high';
    if (confidence >= 0.5) return 'confidence-medium';
    return 'confidence-low';
}

function downloadResults() {
    window.location.href = `${API_BASE}/download/${currentBatchId}`;
}

function resetApp() {
    selectedFiles = [];
    currentBatchId = null;
    fileInput.value = '';
    
    document.querySelector('.upload-section').classList.remove('hidden');
    processingSection.classList.add('hidden');
    resultsSection.classList.add('hidden');
    
    renderFileList();
    progressFill.style.width = '0%';
}
```

---

### Step 9: Testing

#### 9.1 Test OCR Service (`backend/tests/test_ocr.py`)
```python
import pytest
from backend.services.ocr_service import OCRService

@pytest.mark.asyncio
async def test_ocr_extraction():
    ocr = OCRService()  # Will use Tesseract by default
    # Add test with sample PDF
    pass

@pytest.mark.asyncio
async def test_ocr_quality_validation():
    ocr = OCRService()
    
    # Test valid text
    assert ocr.validate_ocr_quality("This is a valid document with enough text to pass validation.")
    
    # Test invalid text
    assert not ocr.validate_ocr_quality("")
    assert not ocr.validate_ocr_quality("abc")
    assert not ocr.validate_ocr_quality("###@@@$$$")
```

#### 9.2 Test AI Service (`backend/tests/test_ai.py`)
```python
import pytest
from backend.services.ai_service import AIService
from backend.models import DocumentCategory

@pytest.mark.asyncio
async def test_invoice_categorization():
    ai = AIService()
    
    sample_invoice_text = """
    INVOICE
    Invoice Number: INV-001
    Date: 01/15/2024
    
    Bill To: ABC Company
    Amount Due: $1,500.00
    """
    
    category, confidence = await ai.categorize_document(sample_invoice_text, "invoice.pdf")
    
    assert category == DocumentCategory.INVOICE
    assert confidence > 0.7

@pytest.mark.asyncio
async def test_category_matching():
    ai = AIService()
    
    assert ai._match_category("Invoice") == DocumentCategory.INVOICE
    assert ai._match_category("contract agreement") == DocumentCategory.CONTRACT
    assert ai._match_category("unknown document") == DocumentCategory.OTHER
```

---

### Step 10: Deployment & Running

#### 10.1 README (`README.md`)
```markdown
# Document Digitization MVP - Phase 1

AI-powered document categorization service that organizes bulk PDF uploads automatically.

## Features
- Bulk PDF upload (up to 100 files)
- OCR text extraction using Tesseract (free, open source)
- AI-powered categorization (9 document types)
- Organized ZIP download
- Processing logs and confidence scores

## Prerequisites
- Python 3.11+
- Tesseract OCR
- OpenAI API account (or Anthropic)

## Setup

### 1. Install System Dependencies

**Ubuntu/Debian:**
```bash
sudo apt-get update
sudo apt-get install tesseract-ocr poppler-utils
```

**macOS:**
```bash
brew install tesseract poppler
```

**Windows:**
- Download Tesseract: https://github.com/UB-Mannheim/tesseract/wiki
- Download Poppler: https://github.com/oschwartz10612/poppler-windows/releases
- Add both to your PATH

### 2. Clone and Install Python Dependencies

```bash
# Clone the repository
git clone <your-repo>
cd document-digitization-mvp

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configure Environment

```bash
cp .env.example .env
# Edit .env with your API keys
```

Required in `.env`:
```bash
OPENAI_API_KEY=sk-...  # Get from platform.openai.com
USE_GOOGLE_VISION=false
```

### 4. Run the Application

```bash
python backend/main.py
```

Open browser to `http://localhost:8000`

## Testing

Run tests:
```bash
pytest backend/tests/
```

## Cost Structure (MVP)

- **OCR**: FREE (Tesseract)
- **AI Categorization**: ~$0.30 per 1,000 documents (GPT-4o-mini)
- **Total**: ~$0.30 per 1,000 documents

**When you're ready to upgrade** (after 2-3 paying customers):
- Switch to Google Vision API for 99% accuracy
- Cost increases to ~$2.00 per 1,000 documents
- Quality improvement: 85-90% ‚Üí 99% accuracy
- Setup takes 5 minutes (see upgrade guide in spec)

## API Endpoints

- `POST /api/upload` - Upload documents
- `GET /api/status/{batch_id}` - Check processing status
- `GET /api/download/{batch_id}` - Download results
- `GET /api/health` - Health check

## Project Structure
See PHASE-1-MVP-SPEC.md for detailed architecture.

## Upgrade Path

Once you have paying customers and want 99% accuracy:

1. Install Google Cloud Vision:
   ```bash
   pip install google-cloud-vision==3.4.5
   ```

2. Set up Google Cloud account and get credentials

3. Update `.env`:
   ```bash
   USE_GOOGLE_VISION=true
   GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json
   ```

4. Restart - done! No code changes needed.

## Next Steps
- Phase 2: Smart indexing and metadata extraction
- Phase 3: DocuWare integration
```

---

## Success Criteria Checklist

### Functional Requirements
- [ ] Accept bulk PDF uploads (1-100 files)
- [ ] Extract text via OCR from scanned documents
- [ ] Categorize documents with 80%+ accuracy
- [ ] Process 50 documents in under 10 minutes
- [ ] Generate organized folder structure
- [ ] Provide downloadable ZIP with results
- [ ] Display processing progress in real-time
- [ ] Show confidence scores for each document
- [ ] Handle errors gracefully with informative messages

### Technical Requirements
- [ ] FastAPI backend with async processing
- [ ] Tesseract OCR integration (with Google Vision upgrade path)
- [ ] OpenAI API integration
- [ ] Background job processing
- [ ] File validation and size limits
- [ ] Clean, intuitive web interface
- [ ] Responsive design
- [ ] API documentation

### Quality Requirements
- [ ] Unit tests for OCR service
- [ ] Unit tests for AI service
- [ ] Integration tests for full pipeline
- [ ] Error handling for all failure modes
- [ ] Logging for debugging
- [ ] Input validation
- [ ] Security: file type validation, size limits

---

## Estimated Timeline

| Task | Estimated Time |
|------|----------------|
| Environment setup (Tesseract install, OpenAI account) | 1-2 hours |
| Backend structure & config | 3-4 hours |
| OCR service implementation | 3-4 hours |
| AI service implementation | 4-6 hours |
| File management service | 3-4 hours |
| API routes & processing pipeline | 6-8 hours |
| Frontend interface | 8-10 hours |
| Testing & debugging | 8-12 hours |
| Documentation | 2-3 hours |
| **Total** | **38-53 hours (1-2 weeks)** |

---

## Cost Estimates (Per 1000 Documents)

| Service | Cost | Notes |
|---------|------|-------|
| Tesseract OCR | **FREE** | Open source, 85-90% accuracy |
| OpenAI GPT-4o-mini | ~$0.30 | Categorization |
| Storage & bandwidth | ~$0.10 | Minimal for MVP |
| **Total per 1000 docs (MVP)** | **~$0.40** | üéâ Almost free! |

**Upgrade Path (After 2-3 Paying Customers):**

| Service | Cost | Notes |
|---------|------|-------|
| Google Vision API OCR | ~$1.50 | Upgrade to 99% accuracy |
| OpenAI GPT-4o-mini | ~$0.30 | Categorization |
| Storage & bandwidth | ~$0.20 | Scales with usage |
| **Total per 1000 docs** | **~$2.00** | Still excellent margins |

**Profit Margins:**
- **MVP Phase**: Charging $5-15 per doc, cost $0.0004 per doc = **99.99% margin** üí∞
- **After Upgrade**: Charging $5-15 per doc, cost $0.002 per doc = **99.96% margin** üí∞

Even with Google Vision, your margins are incredible!

---

## Next Phase Preview

Once Phase 1 MVP is complete and validated:
- **Phase 2**: Add smart indexing with field extraction for specific document types
- **Phase 3**: DocuWare API integration for seamless DMS upload

Focus on getting Phase 1 working perfectly before moving forward!
